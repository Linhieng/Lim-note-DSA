## 🍕 哈希

哈希函数需要有的功能:
- 输入范围: 无穷
- 输出范围: 有穷
    - 比如 MD5, 输出范围就是 [0, $2^{128}-1$], 即 128 位的比特数, 或 32 位的 16 进制数。
- 确定性: 相同输入, 相同输出。 当然, 不同输入也可能有相同输出(碰撞), 但概率极低
- **离散性**, 均匀性
    - 将输出看做一个"域", 因为任意输入值的输出, 在这个域上足够离散, 所以域中的输出, 才足够均匀。 这也是衡量哈希好坏的一个标准。
    - 比如 "域" 是一个圆圈, 每输入一个字符串, 就在域中点上一个点, 即使字符串很相似, 由于哈希的离散型, 他也能把这个点的位置变得很不相同。
    - 所以, 即使你输入的足够多, 这个圆圈上的点也是非常均匀的, 这样才能保证不会"碰撞"
    - 对输出结果进行模运算后, 仍然具备离散性。
        - 因为哈希的输出域太大了, 所以可以通过模运算, 将输出域缩小, 比如与 m 进行模运算, 就能够保证输出范围变成 [0, m-1]。
- 运算的时间复杂度是 `O(1)` 的, 当然, 这个常数比较大, 不像计算 1+1 等于 2 那么快。

哈希的实现方式有很多, 但都需要实现有上面的功能。
简单提一嘴哈希函数的一种实现。 任何数据都是二进制, 以一种编造好的规律, 对每一位都执行操作, 比如这一位执行 &, 另一位执行 |, 又有一个执行 !。

### 案例1 找出现次数最多的数

假设文件中有 40 亿个数字(无符号int), 要求找出出现次数最多的数字。 但内存限制为 1G。

如果直接使用哈希表实现, 假设一条记录占 8 字节, 最差的情况是每个数字都不相同, 此时如何实现哈希表存储, 内存会爆掉。

这个时候就可以利用哈希函数的离散性解决问题。 依次读取每一个数字, 计算他们的哈希值然后对哈希值求模。
比如对 100 求模, 在硬盘创建 100 个文件夹, 硬盘没有空间限制。
每个数字求哈希值再求模, 相同值的放入对应的文件夹中。
这样一来, 当遍历完 40 亿个数字后, 由于哈希函数的离散性, 将能保证 40 亿个数字是近乎均匀的分布在 100 个文件夹中的。
并且因为相同输入一定有相同输出, 所以相同的数字肯定在相同的文件夹中。
此时就可以在每个文件夹中, 利用哈希表求出出现次数最多的数字, 最终比较 100 个文件夹中各自产生的次数最多的数。
从而得出 40 亿个数字中, 哪个数字出现次数最大。

简单的说, 就是分而治之的思维, 但利用哈希函数能够保证这个 "分", "分" 的均匀。

### 经典哈希表原理

【基本原理】:
- 利用哈希函数的离散性, 并且对输出值求模, 然后各自放到一个"桶"里面。
- 一个 "桶" 中的结构是链表结构, 当一个桶中放的元素超过指定的数字 k 时, 就会对哈希表进行扩容。
- 当检测到一个桶的元素超过 k 时, 说明全部通的元素都基本要超过 k 了(均匀性)
- 扩容就是增加哈希表的桶的数量, 同时将原有哈希表中的元素全部重新执行一遍哈希函数, 然后放到新的哈希表中。

【时间复杂度】:
- 哈希函数是 O(1), 计算出哈希值后求模, 同时定位到对应的桶也是 O(1) 级别的, 但是一个桶中最多有 k 个元素, k 是一个常数, 比如设置为 6, 所以在桶中查找元素是也是 O(1)
- 增删改查的时间复杂度都可以认为是 O(1)。重点在于扩容的时间复杂度。 和之前计算堆的扩容类似, 每次都是成倍扩容, 成倍扩容时会重新计算就哈希表中的元素值, 然后放到新哈希表中, 同时桶中的元素会减半。
- 扩容代价, 假设增加到了 N 个字符串, 那么意味扩容了 logN 次, 扩容是每个元素都会重新计算哈希值并求模放到新的哈希表中, 所以拷贝的代价是 O(N), 扩容的总代价就是 O(N logN), 将代价平摊到每一个元素上, 那么每一个元素的扩容代价就是 O(logN)
- 所以, 算上扩容代价时, 增删改查的时间复杂度是 O(logN) 的, 那为什么又认为哈希表的增删改查是 O(1) 呢?
    - 原因时, 只有在需要扩容的时候, 才需要耗费扩容的代价。 如果不是即时性的东西, 完全可以在用户不操作时进行扩容操作, 这样用户 使用 哈希表时增删改查就是 O(1)
    - 此外, 哈希表不会真的从 2 开始扩容, 初始时可以将哈希表定的大一点, 这样小数据时基本不会出发扩容机制。

经典哈希表的原理都是这样的, 但实际使用时的哈希表, 不同语言会有不同的优化, 比如桶中不是链表, 而且数组, 这些优化。

### 案例1 设计 RandomPool 结构


【题目】: 设计一种结构, 在该结构中有如下三个功能
- insert(key): 将某个 key 加入到该结构, 做到不重复加入
- delete(key): 将原本在结构中的某个 key 移除
- getRandom(): 等概率随机返回结构中的任何一个 key

【要求】: Insert、delete和getRandom方法的时间复杂度都是 O(1)

【注意】: 看到题意时, 不要想着是让你自己实现哈希表的结构, 实际上这是一道使用哈希表的题。

单独的插入和删除并不难, 主要是有等概率随机返回一个 key。 key 的值是任意的, 所以我们需要使用一个数字标识它。
比如利用一个 index 来表示每一个 key, index 是连续的, 可以以加入时间次序作为他们的 index。
这样随机时, 只需要在 size 范围上获取一个随机数就可以了。
但又因为会删除 key, 删除 key 时会导致中间镂空, 所以删除的机制是将最后一个数填补到要删除的位置上。
这样就能保证 size 范围内都是有值的。

```java
class Pool<K> {
    HashMap<K, Integer> keyIndexMap; // key 到 index 的 map
    HashMap<Integer, K> indexKeyMap; // index 到 key 的 map
    int size; // index 就是插入的顺序, size 是池中剩余的 key 值, 同时也是 index 值
    // 这一切都是为了 随机 服务的。

    Pool(){}

    void insert(K key) {
        // 不插入重复值, 插入时给的是 key
        if (!this.keyIndexMap.containsKey(key)) {
            this.keyIndexMap.put(key, this.size);
            this.indexKeyMap.put(this.size++, key);
        }
    }
    void delete(K key) {
        if (this.keyIndexMap.containsKey(key)) {
            // 删除方式是, 将最后一个 key 填补到要删除的位置上》
            int deleteIndex = this.keyIndexMap.get(key); // 或者要删除的位置 / 要填补的位置
            int lastIndex = --this.size; // size 从 0 开始, 所以获取最后一个元素时, 下标要 -1
            K lastKey = this.indexKeyMap.get(lastIndex);
            // 将最后位置的值填补到要删除的位置上
            this.keyIndexMap.put(lastKey, deleteIndex);
            this.indexKeyMap.put(deleteIndex, lastKey);
            // 添加完后删除两张表最后的 key 和 index
            this.keyIndexMap.remove(key);
            this.indexKeyMap.remove(lastIndex);

        }
    }
    K getRandom() {
        if (this.size == 0) {
            return null;
        }
        // 创建两张表, 创建 size, 删除时将最后一个元素填补到删除元素为止上, 一切都是为了此刻随机时, 可以直接从 0~size-1 随机出一个数。
        int randomIndex = (int) (Math.random() * this.size);
        return this.indexKeyMap.get(randomIndex);
    }
}
```

### 案例2 布隆过滤器

比如有 40 亿个网站 url 是黑名单, 每次用户访问网站时, 都要知道这个网站是否是黑名单中的。
并且一个网站加入黑名单后不会删除掉, 即我们的需求只有增加和查询两个功能, 不会删除。
这种情形就可以使用布隆过滤器。

布隆过滤器, 其实就是利用哈希函数求取每一个 url 的哈希值, 然后求模得到一个下标, 这个下标所在位置只会是 0/1,
而存储的方式使用的是位图。 即一位代表一个信息。 普通结构存储时, 至少都是一个字节(8位)。 所以使用位图能够节省空间。

布隆过滤器一定会有失误率, 这是由哈希函数决定的, 因为哈希函数会有碰撞, 而你使用布隆过滤器的情况正是因为数据量大。 数据量大, 要想要节省空间, 所以碰撞率就会提高。
失误有两种:
- 网站是黑名单, 但识别为白名单。 布隆过滤器不可能出现这种失误, 因为一个 url 经过哈希函数后, 对应的位图一定值为 true
- 网站是白名单, 误识别为黑名单。 布隆过滤器只会有这种失误。 因为数据量大, 位图空间小, 所以存在某个白名单的 url 计算出来的哈希值求模后, 对应位置的下标是 true。

#### 位图
先看看位图的实现:
- 每一位都是一个信息, 但我们没有 "位" 数组, 我们有的只有普通类型的数组。 所以我们是利用普通类型数组实现位图
- 此时需要实现的就是, 给你一个位图的下标, 你如何在普通类型数组中找到该位的信息
- 同理, 给你一个位图的下标, 你如何修改它的值

```java
int[] arr = new int[10]; // 32bit * 10 这个长度为10的int数组, 我们会将其当成长度为 320 的位图

int i = 178; // 现在有一个位图的下标 178

// 要如何查询和修改对应位上的数据?
// 需要两个值, 这个 "位" , 在 arr 数组中的第几个元素中
int numIndex = 178 / 32; // arr 数组一个元素里面有 32 位, 所以 numIndex 就是所在 arr 数组下标
// 找到在那个元素里面, 那么他是在 32 位中的第几位呢?
int bitIndex = 178 % 32; // 通过模运算就可以得出第几位

// 有了这两个信息后, 就可以查询对应位的值了
// 首先, 取出第 numIndex 个元素, 因为值在该元素中, 然后右移 bitIndex 位, 此时最右侧的值就是我们要的信息了, 将它与 1 & 一下, 就知道这个位上的值是 0 是  1 了
int val = (  (arr[numIndex] >> bitIndex)  &  1 )

// 修改位上的值, 如果要改成 1, 也就是要让其他位保持不变, 只有对应位始终为 1, 此时可以使用 | 运算
// | 运算,  任意值 | 1 = 1, 任意值 | 0 = 原值。 所以我们要将对应位上的值改为 1, 其他位上的值改为 0
arr[numIndex] = arr[numIndex] | (1 << bitIndex); // 先将 1 移到对应位上, 此时然后 | 上对应元素, 这样就实现了只有对应值变成 1, 其他位的值都是原值

// 同理, 要将位上信息该位 1, 可以使用 & 运算, 然后只让该位的值为 0
arr[numIndex] = arr[numIndex] & (~ (1 << bitIndex))
```

#### 布隆过滤器

【失误率分析】:
- 布隆过滤器的失误率有两个因素: 样本量 n, 位图大小 m
    - 位图大小 m。 求出哈希值后, 需要将哈希值映射到 m 上
    - 样本量 n, 单个样本具体的长度是无所谓的, 因为哈希算法不限制输入范围。
- 假设 n 不变, 如果位图太小, 那么位图上的格子很容易全被描黑。 (格子表示某一位, 描黑表示该值为黑名单的值)
    - 这样当非黑名单的 url 映射到位图上, 很容易掉入被描黑的格子中, 失误率就会高
- 假设 m 不变, n 太大, 同样失误率也容易高。

【步骤】:
- 利用 k 个哈希生成器, 对每一个 url 对计算出对应的哈希值, 然后对位图大小 m 求模, 得出多个位图下标
- 当查询时, 同样会利用 k 个哈希生成器处理 url, 然后求模, 最终得到多个下标, 查询位图上对应下标是否描黑
    - 虽然使用了多个哈希生成器, 但如果 url 是黑名单, 那么对应的多个格子, 肯定都是被描黑的
    - 但如果 url 是白名单时, 出现误判时, 大多数情况下会是一些被描黑, 一些没被描黑, 小概率出现全被描黑的情况
    - 所以使用多个哈希生成器时, 可以控制失误率

【控制失误率】:
- 我们能改变的参数有两个, 一个是 k, 即选用多少个哈希生成器, 一个是 m, 即位图定义多大。
- 首先, 只有 m 变化时, 当 m 增大时, 失误率会是下降的, 并且曲线会是反过来的 log 函数曲线 ╰
- 然后, 只有 k 固定时, 对应的失误率曲线应该是先减后增的。 后面会增加是由于, 当你使用太多哈希生成器时, 每一个 url 会生成 k 个格子, k 变大时, 位图很容易被填满, 所以失误率后续会上升。
